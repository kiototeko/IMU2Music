# IMU2Music: Learning to Generate Music from IMU Sensor Readings

In this project we will be exploring a way to generate music given IMU readings from a person playing an instrument.

## Project Goal and Vision

Explore different neural network architectures in the task of generating music from IMU readings present in performers' bodies.

With the advent of diverse neural network architectures for the task of processing time sequences, in the form of Recurrent Neural Networks (RNNs) and the variants that have been created since its inception, like Long Short-Term Memory (LSTM) architectures and transformers, among others, a plethora of uses have been engineered for these types of networks that go beyond classification tasks, like machine translation which convert one sequence of inputs into another, usually language-related. A further development has seen the creation of networks that translate a sequence from one sensor modality to another, which is the task we are undertaking in this project.

## Specific Aims

1. Obtain a dataset that captures musicians motion while performing, using Intertial Motion Sensors.
2. Design an extension to the musical transformer network used in both \[[4](#references)\] and \[[8](#references)\] so as to include IMU data
3. Test its accuracy compared to the previous networks

## Previous Work

Previous work has dealt with translating from images to speech, as in \[[1](#references)\], where an image is translated into a spoken description of it via a transformer that uses text as an intermediate modality or translating video to sound, like in \[[5](#references)\], where for videos with natural occurrences a sound is produced using an RNN. In contrast, in \[[2](#references)\], the cross-modality is done between speech and face images, where the task is to produce a biometric system that identifies individuals based on one of this modalities. More interesting work has been that done in MIT's Computer Science and Artificial Intelligence Laboratory, where, for example, in \[[3](#references)\], given a robot arm equipped with a touch sensor and a camera recording the robot arm's motion, a bidirectional correspondence between these modalities is established by using conditional GANs. More related to our current task, in \[[4](#references)\] they are able to translate video recordings of musicians performing into the actual sound one would expect hearing, by means of a transformer. Finally, we have work like in \[[6](#references)\] where the objective is to translate an audio recording of a musician performing into an animation of it, with the body dynamics included. This could be thought as the inverse of the task we are exploring in this work, whereas in here they use an LSTM network trained on videos, where keypoints are extracted from the bodies shown. Our work instead uses only IMU readings and tries to translate those into the respective audio recording.

## Technical Approach

The approach taken in here follows that used in Foley (\[[4](#references)\]), where the task again was to translate body movements extracted from silenced video recordings of musicians playing their instruments into sound sequences that try to resemble the original ones. For this purpose they use a transformer architecture based on previous work (\[[8](#references)\]), although only the decoder part is used as such, the encoder is replaced by a Graph CNN that captures the body dynamics of the performers based on previously extracted sequences of body keypoints. While the encoder does need to be different in our case, as we are using IMU readings instead, the decoder part is of interest to us as it is the part where the sound sequences are generated. Sound is represented in the Musical Instrument Digital Interface (MIDI) representation, which helps the model learn the rules contained in music as it encodes sound as a sequence of structured events where timing, pitch and velocity (hardness of the note played) are taken into account. By using this representation, we can input a sequence of MIDI events into the transformer decoder and coupled with the motion information, predict a new MIDI event. The transformer architecture is specially helpful because of the self-attention modules which, given the common presence of recurrent elements in musical pieces that can repeat after large intervals of time, are able to take all of these relations into account. Finally, the original transformer architecture uses absolute position encoded into the input embeddings, but in this case relative positional representation is adopted to make sure the self-attention module takes into account the distance between nearby tokens, which is an advantage when dealing with music, as relative differences in timing and pitch matter more than absolute ones.

## Implementation

The first challenge in this work was searching for a dataset where IMU readings were taken from musicians playing their instruments. Unfortunately, there is very little in this respect, i.e., there are datasets with IMU readings but applied to human activity recognition, not to musical tasks, or in the cases were musical activity is measured, either optical motion capture systems are used, or in the case of \[[9](#references)\], the IMU readings only comprise one of the arms of the subject, so that not all musical properties can be infered. Thankfully, a dataset was found which consisted of musicians playing violin, where both electromyograph and IMU readings are taken, as well as optical motion capture and audio recordings. This dataset was collected in the context of the Technology Enhanced Learning of Music Instruments ([TELMI](http://telmi.upf.edu/)) project from the Music Technology Group of the Universitat Pompeu Fabra in Barcelona, which wanted to enhance musical learning by developing a suite of technology that guides students through that process. The problem is that the data is stored on separate files that link to RepoVizz, a multimodal online database and visualization tool, which requires some user interaction to choose which data streams to download from each file, instead of providing all the database in one file. This was solved by writing a small python script using Selenium that automates this retrieval of data. Because the transformer decoder expects MIDI as its input, the audio files were converted into MIDI files by use of Wave-to-Notes transcriber ([Waon](https://github.com/kichiki/WaoN)), which can let you also specify the pitch range we want, useful to limit the number of variables to process. Before this certain processing was needed to clip audio files and IMU readings to remove both the initial and final silence intervals, as in those moments there is now music playing and there is no meaning to the readings obtained. Unfortunately, at the end of this process, it was revealed that the resulting dataset only contained in total 47.8 minutes of data, which when splitted into samples of 6 seconds to take as input to the model, resulted in only 478 samples in total, which is very small indeed. We are able to augment data by randomizing the initial start time of the samples, but nevertheless it is still small. To try to overcome this limitation, we train the original model in \[[4](#references)\], so as to perform transfer learning and facilitate the process of adaptation to the new modality.


## Future Directions

Given the tiny amount of data available to train a model using both IMU sensors and audio of musicians performing, an interesting future direction could deal with generating the IMU readings directly from videos of musicians so as to surpass this problem. In fact, in \[[7](#references)\] they do exactly that, although for the purpose of human activity recognition, where they test both an LSTM and a Random Forest model. Unfortunately, their code is not open to the public so we are not able to test it for now.

## References

\[1\] Ma, S., Mcduff, D., & Song, Y. (2019, October). Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck. 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 2019 IEEE/CVF International Conference on Computer Vision (ICCV). [https://doi.org/10.1109/iccv.2019.00769 ](https://doi.org/10.1109/iccv.2019.00769)


\[2\] Arsha Nagrani, Samuel Albanie, Andrew Zisserman. Seeing Voices and Hearing Faces: Cross-Modal Biometric Matching (2018). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8427-8436. [https://openaccess.thecvf.com/content_cvpr_2018/html/Nagrani_Seeing_Voices_and_CVPR_2018_paper.html](https://openaccess.thecvf.com/content_cvpr_2018/html/Nagrani_Seeing_Voices_and_CVPR_2018_paper.html)

\[3\] Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, Antonio Torralba. Connecting Touch and Vision via Cross-Modal Prediction (2019). Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 10609-10618. [https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Connecting_Touch_and_Vision_via_Cross-Modal_Prediction_CVPR_2019_paper.html](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Connecting_Touch_and_Vision_via_Cross-Modal_Prediction_CVPR_2019_paper.html)

\[4\] Chuang Gan and Deng Huang and Peihao Chen and Joshua B. Tenenbaum and Antonio Torralba (2020). Foley Music: Learning to Generate Music from Videos. [https://arxiv.org/abs/2007.10984](https://arxiv.org/abs/2007.10984)

\[5\] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara L. Berg (2018). Visual to Sound: Generating Natural Sound for Videos in the Wild. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3550-3558. [https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Visual_to_Sound_CVPR_2018_paper.html](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Visual_to_Sound_CVPR_2018_paper.html)

\[6\] Eli Shlizerman, Lucio Dery, Hayden Schoen, Ira Kemelmacher-Shlizerman (2018). Audio to Body Dynamics. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 7574-7583. [https://openaccess.thecvf.com/content_cvpr_2018/html/Shlizerman_Audio_to_Body_CVPR_2018_paper.html](https://openaccess.thecvf.com/content_cvpr_2018/html/Shlizerman_Audio_to_Body_CVPR_2018_paper.html)

\[7\] Kwon, H., Tong, C., Haresamudram, H., Gao, Y., Abowd, G. D., Lane, N. D., & Plötz, T. (2020). IMUTube. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1–29. [https://doi.org/10.1145/3411841](https://doi.org/10.1145/3411841)

\[8\] Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Noam Shazeer and Ian Simon and Curtis Hawthorne and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck (2018). Music Transformer. [https://arxiv.org/abs/1809.04281](https://arxiv.org/abs/1809.04281)

\[9\] Sarasúa, Á., Caramiaux, B., Tanaka, A., & Ortiz, M. (2017, June 28). Datasets for the Analysis of Expressive Musical Gestures. Proceedings of the 4th International Conference on Movement Computing. MOCO ’17: 4th International Conference on Movement Computing. [https://doi.org/10.1145/3077981.3078032](https://doi.org/10.1145/3077981.3078032)
